{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install evaluate\n#!pip install datasets\n!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:19:36.493114Z","iopub.execute_input":"2024-04-23T08:19:36.493832Z","iopub.status.idle":"2024-04-23T08:19:48.725375Z","shell.execute_reply.started":"2024-04-23T08:19:36.493785Z","shell.execute_reply":"2024-04-23T08:19:48.724317Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T08:15:14.597601Z","iopub.execute_input":"2024-04-23T08:15:14.597874Z","iopub.status.idle":"2024-04-23T08:15:33.887061Z","shell.execute_reply.started":"2024-04-23T08:15:14.597848Z","shell.execute_reply":"2024-04-23T08:15:33.886207Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-04-23 08:15:23.023147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-23 08:15:23.023286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-23 08:15:23.150899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"ner_datasets = load_dataset(\"msra_ner\", cache_dir=\"./data\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:33.888208Z","iopub.execute_input":"2024-04-23T08:15:33.888855Z","iopub.status.idle":"2024-04-23T08:15:45.590323Z","shell.execute_reply.started":"2024-04-23T08:15:33.888827Z","shell.execute_reply":"2024-04-23T08:15:45.589416Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 4.34M/4.34M [00:02<00:00, 1.68MB/s]\nDownloading data: 100%|██████████| 339k/339k [00:00<00:00, 414kB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d4f41f87abd469d9afd0c6ba076e2fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4246c35752934b868f883d49193c863d"}},"metadata":{}}]},{"cell_type":"code","source":"ner_datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:45.592549Z","iopub.execute_input":"2024-04-23T08:15:45.592835Z","iopub.status.idle":"2024-04-23T08:15:45.599524Z","shell.execute_reply.started":"2024-04-23T08:15:45.592811Z","shell.execute_reply":"2024-04-23T08:15:45.598627Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(ner_datasets[\"train\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:45.600746Z","iopub.execute_input":"2024-04-23T08:15:45.601017Z","iopub.status.idle":"2024-04-23T08:15:45.611964Z","shell.execute_reply.started":"2024-04-23T08:15:45.600994Z","shell.execute_reply":"2024-04-23T08:15:45.611124Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{'id': '0', 'tokens': ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"code","source":"ner_datasets[\"train\"].features","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:45.612966Z","iopub.execute_input":"2024-04-23T08:15:45.613249Z","iopub.status.idle":"2024-04-23T08:15:45.624614Z","shell.execute_reply.started":"2024-04-23T08:15:45.613228Z","shell.execute_reply":"2024-04-23T08:15:45.623685Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'id': Value(dtype='string', id=None),\n 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}"},"metadata":{}}]},{"cell_type":"code","source":"label_list = ner_datasets[\"train\"].features[\"ner_tags\"].feature.names\nlabel_list","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:45.625612Z","iopub.execute_input":"2024-04-23T08:15:45.625862Z","iopub.status.idle":"2024-04-23T08:15:45.636809Z","shell.execute_reply.started":"2024-04-23T08:15:45.625841Z","shell.execute_reply":"2024-04-23T08:15:45.636006Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:45.637808Z","iopub.execute_input":"2024-04-23T08:15:45.638066Z","iopub.status.idle":"2024-04-23T08:15:49.603015Z","shell.execute_reply.started":"2024-04-23T08:15:45.638045Z","shell.execute_reply":"2024-04-23T08:15:49.602002Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d22ae264bdb461eab5b9cd6440b3595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8aba7399f8c455c990468f1de9e8d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68882d3f6fbf40ee84bffc2f7e5f593e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0272705193534e908bc4ce45edc90966"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer(ner_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)   # 对于已经做好tokenize的数据，要指定is_split_into_words参数为True","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:49.604353Z","iopub.execute_input":"2024-04-23T08:15:49.604725Z","iopub.status.idle":"2024-04-23T08:15:49.612988Z","shell.execute_reply.started":"2024-04-23T08:15:49.604697Z","shell.execute_reply":"2024-04-23T08:15:49.612040Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer(ner_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True).word_ids())","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:49.617514Z","iopub.execute_input":"2024-04-23T08:15:49.617791Z","iopub.status.idle":"2024-04-23T08:15:49.695975Z","shell.execute_reply.started":"2024-04-23T08:15:49.617770Z","shell.execute_reply":"2024-04-23T08:15:49.695010Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, None]\n","output_type":"stream"}]},{"cell_type":"code","source":"res = tokenizer(\"interesting word\")\nres","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:49.697217Z","iopub.execute_input":"2024-04-23T08:15:49.697949Z","iopub.status.idle":"2024-04-23T08:15:49.706283Z","shell.execute_reply.started":"2024-04-23T08:15:49.697914Z","shell.execute_reply":"2024-04-23T08:15:49.705420Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 10673, 12865, 12921, 8181, 8681, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"res.word_ids()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:49.707515Z","iopub.execute_input":"2024-04-23T08:15:49.707791Z","iopub.status.idle":"2024-04-23T08:15:49.716844Z","shell.execute_reply.started":"2024-04-23T08:15:49.707768Z","shell.execute_reply":"2024-04-23T08:15:49.716000Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[None, 0, 0, 0, 0, 1, None]"},"metadata":{}}]},{"cell_type":"code","source":"# 借助word_ids 实现标签映射\ndef process_function(examples):\n    tokenized_exmaples = tokenizer(examples[\"tokens\"], max_length=128, truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_exmaples.word_ids(batch_index=i)\n        label_ids = []\n        for word_id in word_ids:\n            if word_id is None:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_id])\n        labels.append(label_ids)\n    tokenized_exmaples[\"labels\"] = labels\n    return tokenized_exmaples","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:49.717965Z","iopub.execute_input":"2024-04-23T08:15:49.718315Z","iopub.status.idle":"2024-04-23T08:15:49.726353Z","shell.execute_reply.started":"2024-04-23T08:15:49.718253Z","shell.execute_reply":"2024-04-23T08:15:49.725601Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"这段代码是一个处理函数，用于对给定的例子进行标记映射。它接受一个包含文本和相应命名实体识别（NER）标签的字典作为输入，并返回一个包含标记化文本和映射标签的字典。以下是该函数的详细解释：\n1. `tokenizer`：这是一个标记器函数，用于将文本转换为标记。它接受文本、最大长度、截断标志和是否按单词分割的标志作为输入。\n2. `tokenized_exmaples = tokenizer(examples[\"tokens\"], max_length=128, truncation=True, is_split_into_words=True)`：这行代码使用标记器函数对输入文本进行标记化处理，并将结果存储在`tokenized_exmaples`变量中。最大长度设置为128，如果文本长度超过128，则进行截断。同时，标记器会按单词进行分割。\n3. `labels = []`：这是一个空列表，用于存储映射后的标签。\n4. `for i, label in enumerate(examples[\"ner_tags\"]):`：这行代码遍历输入字典中的NER标签，并为每个标签分配一个索引`i`。\n5. `word_ids = tokenized_exmaples.word_ids(batch_index=i)`：这行代码获取标记化文本中的单词ID。`word_ids`是一个列表，其中包含与每个标记对应的单词ID。如果标记是一个子单词，则其单词ID与其前一个标记的单词ID相同；如果标记是一个特殊标记（如CLS或SEP），则其单词ID为None。\n6. `label_ids = []`：这是一个空列表，用于存储映射后的标签ID。\n7. `for word_id in word_ids:`：这行代码遍历`word_ids`列表中的每个单词ID。\n8. `if word_id is None:`：这行代码检查单词ID是否为None。如果是，说明当前标记是一个特殊标记，我们将标签ID设置为-100。\n9. `else:`：如果单词ID不是None，说明当前标记是一个单词的一部分。我们将输入标签中的相应标签ID添加到`label_ids`列表中。\n10. `labels.append(label_ids)`：这行代码将`label_ids`列表添加到`labels`列表中。\n11. `tokenized_exmaples[\"labels\"] = labels`：这行代码将`labels`列表添加到标记化示例字典中。\n12. `return tokenized_exmaples`：这行代码返回包含标记化文本和映射标签的字典。\n总之，这个处理函数使用标记器对输入文本进行标记化处理，并将输入标签映射到标记化文本上。映射后的标签将用于后续的命名实体识别任务。\n","metadata":{}},{"cell_type":"code","source":"tokenized_datasets = ner_datasets.map(process_function, batched=True)\ntokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:15:49.727252Z","iopub.execute_input":"2024-04-23T08:15:49.727483Z","iopub.status.idle":"2024-04-23T08:16:04.573551Z","shell.execute_reply.started":"2024-04-23T08:15:49.727463Z","shell.execute_reply":"2024-04-23T08:16:04.572643Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95bfb87d74e341cb9827cd6a3a467d0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b056bf4e169044bf8f4487490b7dbfd5"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 45001\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3443\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenized_datasets[\"train\"][5])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:16:04.574734Z","iopub.execute_input":"2024-04-23T08:16:04.575507Z","iopub.status.idle":"2024-04-23T08:16:04.581088Z","shell.execute_reply.started":"2024-04-23T08:16:04.575480Z","shell.execute_reply":"2024-04-23T08:16:04.580082Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"{'id': '5', 'tokens': ['我', '们', '是', '受', '到', '郑', '振', '铎', '先', '生', '、', '阿', '英', '先', '生', '著', '作', '的', '启', '示', '，', '从', '个', '人', '条', '件', '出', '发', '，', '瞄', '准', '现', '代', '出', '版', '史', '研', '究', '的', '空', '白', '，', '重', '点', '集', '藏', '解', '放', '区', '、', '国', '民', '党', '毁', '禁', '出', '版', '物', '。'], 'ner_tags': [0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 2769, 812, 3221, 1358, 1168, 6948, 2920, 7195, 1044, 4495, 510, 7350, 5739, 1044, 4495, 5865, 868, 4638, 1423, 4850, 8024, 794, 702, 782, 3340, 816, 1139, 1355, 8024, 4730, 1114, 4385, 807, 1139, 4276, 1380, 4777, 4955, 4638, 4958, 4635, 8024, 7028, 4157, 7415, 5966, 6237, 3123, 1277, 510, 1744, 3696, 1054, 3673, 4881, 1139, 4276, 4289, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, -100]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# 对于所有的非二分类任务，切记要指定num_labels，否则就会device错误\nmodel = AutoModelForTokenClassification.from_pretrained(\"bert-base-chinese\", num_labels=len(label_list))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:16:04.582309Z","iopub.execute_input":"2024-04-23T08:16:04.582628Z","iopub.status.idle":"2024-04-23T08:16:13.977950Z","shell.execute_reply.started":"2024-04-23T08:16:04.582604Z","shell.execute_reply":"2024-04-23T08:16:13.977213Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc4afb4acbc54468b685cc6c4b90f7ff"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.num_labels","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:16:13.979067Z","iopub.execute_input":"2024-04-23T08:16:13.979380Z","iopub.status.idle":"2024-04-23T08:16:13.985784Z","shell.execute_reply.started":"2024-04-23T08:16:13.979340Z","shell.execute_reply":"2024-04-23T08:16:13.984608Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"7"},"metadata":{}}]},{"cell_type":"code","source":"# 这里方便大家加载，替换成了本地的加载方式，无需额外下载\nseqeval = evaluate.load(\"seqeval\")\nseqeval","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:19:58.489540Z","iopub.execute_input":"2024-04-23T08:19:58.490518Z","iopub.status.idle":"2024-04-23T08:19:59.603992Z","shell.execute_reply.started":"2024-04-23T08:19:58.490478Z","shell.execute_reply":"2024-04-23T08:19:59.603078Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\nProduces labelling scores along with its sufficient statistics\nfrom a source against one or more references.\n\nArgs:\n    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n    references: List of List of reference labels (Ground truth (correct) target values)\n    suffix: True if the IOB prefix is after type, False otherwise. default: False\n    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n        default: None\n    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n        If you want to only count exact matches, pass mode=\"strict\". default: None.\n    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n        \"warn\". \"warn\" acts as 0, but the warning is raised.\n\nReturns:\n    'scores': dict. Summary of the scores for overall and per type\n        Overall:\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1': F1 score, also known as balanced F-score or F-measure,\n        Per type:\n            'precision': precision,\n            'recall': recall,\n            'f1': F1 score, also known as balanced F-score or F-measure\nExamples:\n\n    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n    >>> seqeval = evaluate.load(\"seqeval\")\n    >>> results = seqeval.compute(predictions=predictions, references=references)\n    >>> print(list(results.keys()))\n    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n    >>> print(results[\"overall_f1\"])\n    0.5\n    >>> print(results[\"PER\"][\"f1\"])\n    1.0\n\"\"\", stored examples: 0)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef eval_metric(pred):\n    predictions, labels = pred\n    predictions = np.argmax(predictions, axis=-1)\n\n    # 将id转换为原始的字符串类型的标签\n    true_predictions = [\n        [label_list[p] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels) \n    ]\n\n    true_labels = [\n        [label_list[l] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels) \n    ]\n\n    result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")\n\n    return {\n        \"f1\": result[\"overall_f1\"]\n    }\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:20:00.527625Z","iopub.execute_input":"2024-04-23T08:20:00.528822Z","iopub.status.idle":"2024-04-23T08:20:00.536499Z","shell.execute_reply.started":"2024-04-23T08:20:00.528777Z","shell.execute_reply":"2024-04-23T08:20:00.535443Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"这段代码定义了一个评估指标函数`eval_metric`，用于评估序列标注模型在命名实体识别（NER）任务上的性能。函数的输入是一个元组`pred`，其中包含模型预测的分数和真实的标签。代码使用`numpy`库来处理数组运算，并使用`seqeval`库来计算评估指标。\n以下是代码的详细解释：\n1. `import numpy as np`：导入`numpy`库，用于执行高效的数学运算。\n2. `def eval_metric(pred):`：定义一个名为`eval_metric`的函数，它接受一个参数`pred`，这是一个包含模型预测分数和真实标签的元组。\n3. `predictions, labels = pred`：将`pred`元组解包为两个变量`predictions`和`labels`，分别存储模型预测的分数和真实的标签。\n4. `predictions = np.argmax(predictions, axis=-1)`：使用`numpy`的`argmax`函数沿最后一个轴（即类别轴）找到每个样本的最大预测分数的索引，这些索引对应于预测的标签ID。\n5. `true_predictions = [...]`：这是一个列表推导式，用于将预测的标签ID转换为原始的字符串类型的标签。对于每个预测和标签序列，它遍历它们并创建一个新的列表，其中只包含标签不是-100的元素。`label_list[p]`用于将标签ID转换为字符串标签。\n6. `true_labels = [...]`：这是另一个列表推导式，与`true_predictions`类似，但它用于转换真实的标签ID为字符串标签。\n7. `result = seqeval.compute(predictions=true_predictions, references=true_labels, mode=\"strict\", scheme=\"IOB2\")`：这行代码使用`seqeval`库的`compute`函数来计算评估指标。`true_predictions`是模型的预测，`true_labels`是真实的标签。`mode`参数设置为\"strict\"，表示严格评估模式，`scheme`参数设置为\"IOB2\"，表示使用IOB2标签格式。\n8. `return {\"f1\": result[\"overall_f1\"]}`：函数返回一个字典，其中包含整体的F1分数，这是NER任务中常用的评估指标。\n总之，这个函数用于评估模型在NER任务上的性能，它将模型输出的分数转换为标签，并使用`seqeval`库来计算F1分数。\n","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"models_for_ner\",\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=128,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    metric_for_best_model=\"f1\",\n    load_best_model_at_end=True,\n    logging_steps=50,\n    num_train_epochs=1,\n    report_to=['tensorboard']\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:20:01.364031Z","iopub.execute_input":"2024-04-23T08:20:01.364711Z","iopub.status.idle":"2024-04-23T08:20:01.443421Z","shell.execute_reply.started":"2024-04-23T08:20:01.364678Z","shell.execute_reply":"2024-04-23T08:20:01.442595Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom accelerate.data_loader import DataLoaderConfiguration\n\n# 创建一个DataLoaderConfiguration实例并设置相应的参数\ndataloader_config = DataLoaderConfiguration(\n    dispatch_batches=None,\n    split_batches=False,\n    even_batches=True,\n    use_seedable_sampler=True\n)\n\n# 现在使用DataLoaderConfiguration实例来初始化Accelerator\naccelerator = Accelerator(dataloader_config=dataloader_config)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    compute_metrics=eval_metric,\n    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:20:02.144141Z","iopub.execute_input":"2024-04-23T08:20:02.144979Z","iopub.status.idle":"2024-04-23T08:20:02.446435Z","shell.execute_reply.started":"2024-04-23T08:20:02.144946Z","shell.execute_reply":"2024-04-23T08:20:02.445606Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:20:03.900403Z","iopub.execute_input":"2024-04-23T08:20:03.901228Z","iopub.status.idle":"2024-04-23T08:28:32.183263Z","shell.execute_reply.started":"2024-04-23T08:20:03.901198Z","shell.execute_reply":"2024-04-23T08:28:32.182261Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='704' max='704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [704/704 08:26, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.020900</td>\n      <td>0.020928</td>\n      <td>0.946303</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=704, training_loss=0.04033736163877289, metrics={'train_runtime': 507.8956, 'train_samples_per_second': 88.603, 'train_steps_per_second': 1.386, 'total_flos': 2832423606981672.0, 'train_loss': 0.04033736163877289, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:28:42.026241Z","iopub.execute_input":"2024-04-23T08:28:42.026634Z","iopub.status.idle":"2024-04-23T08:28:58.829388Z","shell.execute_reply.started":"2024-04-23T08:28:42.026604Z","shell.execute_reply":"2024-04-23T08:28:58.828428Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [27/27 00:12]\n    </div>\n    "},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.02092777192592621,\n 'eval_f1': 0.9463030643800956,\n 'eval_runtime': 16.7905,\n 'eval_samples_per_second': 205.056,\n 'eval_steps_per_second': 1.608,\n 'epoch': 1.0}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\n# 使用pipeline进行推理，要指定id2label\nmodel.config.id2label = {idx: label for idx, label in enumerate(label_list)}\nmodel.config","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:29:03.743951Z","iopub.execute_input":"2024-04-23T08:29:03.744317Z","iopub.status.idle":"2024-04-23T08:29:03.752327Z","shell.execute_reply.started":"2024-04-23T08:29:03.744289Z","shell.execute_reply":"2024-04-23T08:29:03.751425Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"_name_or_path\": \"bert-base-chinese\",\n  \"architectures\": [\n    \"BertForTokenClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"directionality\": \"bidi\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"O\",\n    \"1\": \"B-PER\",\n    \"2\": \"I-PER\",\n    \"3\": \"B-ORG\",\n    \"4\": \"I-ORG\",\n    \"5\": \"B-LOC\",\n    \"6\": \"I-LOC\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.39.3\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 21128\n}"},"metadata":{}}]},{"cell_type":"code","source":"# 如果模型是基于GPU训练的，那么推理时要指定device\n# 对于NER任务，可以指定aggregation_strategy为simple，得到具体的实体的结果，而不是token的结果\nner_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:29:07.636826Z","iopub.execute_input":"2024-04-23T08:29:07.637543Z","iopub.status.idle":"2024-04-23T08:29:07.645703Z","shell.execute_reply.started":"2024-04-23T08:29:07.637511Z","shell.execute_reply":"2024-04-23T08:29:07.644803Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"res = ner_pipe(\"马云在杭州创建了阿里巴巴\")\nres","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:29:08.832500Z","iopub.execute_input":"2024-04-23T08:29:08.833125Z","iopub.status.idle":"2024-04-23T08:29:08.879361Z","shell.execute_reply.started":"2024-04-23T08:29:08.833091Z","shell.execute_reply":"2024-04-23T08:29:08.878055Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[{'entity_group': 'PER',\n  'score': 0.9968899,\n  'word': '马 云',\n  'start': 0,\n  'end': 2},\n {'entity_group': 'LOC',\n  'score': 0.99767697,\n  'word': '杭 州',\n  'start': 3,\n  'end': 5},\n {'entity_group': 'ORG',\n  'score': 0.98138344,\n  'word': '阿 里 巴 巴',\n  'start': 8,\n  'end': 12}]"},"metadata":{}}]},{"cell_type":"code","source":"# 根据start和end取实际的结果\nner_result = {}\nx = \"马云在杭州创建了阿里巴巴\"\nfor r in res:\n    if r[\"entity_group\"] not in ner_result:\n        ner_result[r[\"entity_group\"]] = []\n    ner_result[r[\"entity_group\"]].append(x[r[\"start\"]: r[\"end\"]])\n\nner_result","metadata":{"execution":{"iopub.status.busy":"2024-04-23T08:29:36.393673Z","iopub.execute_input":"2024-04-23T08:29:36.394030Z","iopub.status.idle":"2024-04-23T08:29:36.401698Z","shell.execute_reply.started":"2024-04-23T08:29:36.394006Z","shell.execute_reply":"2024-04-23T08:29:36.400711Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'PER': ['马云'], 'LOC': ['杭州'], 'ORG': ['阿里巴巴']}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}